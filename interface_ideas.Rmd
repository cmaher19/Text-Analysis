---
title: "interface_ideas"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

User Input:
  - They choose a file from their computer and somehow my function determines which files type it is and then reads it in accordingly
    - When they choose a file to upload, does R receive the whole file pathway or how does that work? Does uploading the file upload it into R?
    - Ask them to specify whether they are reading in one file or multiple files?
      - Just one example of there needing to be a differentiation: if you only have one .txt file to read in, you need to specify a header arguments whereas the header argument throws an error if you have multiple .txt files (a corpus)
    - What type of file is it? Is it a book where we'd want to create a line number variables?
      - Are there other types of documents where we'd want to preserve some of the metadata?
        - ex: may want to break down by the date a news article was published if you have a bunch over the course of some period of time
          - same thing goes for tweets - how does word usage change over time?
    - Think about having some feature where they can enter their Twitter username and it will pull their tweets
      - I'm not sure if this is possible with the API but something like it could be possible with the Twitter google docs page I found (FIND LINK) that pulls tweets automatically every hour
    - Web scraping: should we have a place where they can paste a URL and we draw in the file from there?
      - Questions about wha text to take from a webpage
        - Just html paragraph elements? - this would leave out bulleted lists that can contain important information but it is an easy way to do it
  - Display raw data to user
    - To make tokens from a csv file (or really any file with more than one column of data), the user needs to know which column to tokenize
    - Are there words that we want to add to the stop word list (have them removed) OR that we want to keep (and would have otherwise been removed by the stop words)?
      - Will they be able to see the stop word list beforehand?
    - At the heart of it, we want one word (or two for bigrams, etc) per line and some variable to act as an index (line, chapter, section, etc)
  - Actual visualizations: which ones do we want?
    - For books: would we want to be able to break it up by chapter?
      - Have some feature where you can choose what to split it up by (lines or chapters) so that we can create graphs like the Peter Pan sentiment graphs
    - Word frequencies, word clouds, sentiment analysis, network graphs for concurrence and correlation, topic modeling
  - Sentiment analysis
    - Do we want to ahve a couple of options of lexicons for the user to choose from (and compare between)? Do we want to use just the ones I've been using or should I find some more specialized ones to offer as well (finance lexicons, etc)
    - Have an option to specify words to exclude (like Darling in Peter Pan)
      - Will they be able to see the lexicon beforehand?
      - Should there be some prompting for the user to actually look at the output and see that the words make sense?
    - http://jacobsimmering.com/2016/11/15/tidytext/ this guy did unique removal and also just has more good examples of what you can do with the tidytext package
  - What types of explanations should I have throughout the document?
    - A description of text analysis as a whole and how it canbe useful to us
    - Maybe things you should know about your data before you begin?
      - What do you want to tokenize?
      - Do you have variables? Or is your file plain text?
      - Are there specific words that you should be weary of throughout your analysis?
    - Sentiment analysis
      - Will need to explain sentiment lexicons
      - Some description or information about breaking documents down into sections for analysis (by chapter or groups of lines, etc)
    - Tf-idf score
      - May want to look back at resources from Michael Barlow (linguistics guy at UoA that Chris and I met with) and see which statistics they use to measure a term's "importance"
    - Further things you can do with text analysis and its application to research/academics
  - Which features are most important to include?
    - Word frequency types of things: bar charts, scatterplots, wordclouds
    - Identifying themes throughout the document: can show clustering through bar charts and boxplots
    - Sentiment analysis: tons of different types of bar charts
    - Tf-idf scores across documents: bar charts
    - Correlation and cooccurrence: bar charts and network graphs
    
    
    
Additional Notes
- What questions are important in terms of the goals of iNZight?
    - 1. make things really easy for people to get and see things that are useful to them
    - 2. isolate places where human input is required and try to automate everything else
    - 3. ease transition from point and click system to coding complex procedures
- How would be measure ‘effective’ learning?
    - Ease of use
    - Think about: what did I want the module to teach the user? Did it work?
    - Action Research - Stephanie Budgett
- Possible Resources
  - Statistics Education Research Journal
    - Good for research on students
  - Journal of Statistics Education
  
  
  
What formats can the data come in?
  - File types: txt, csv, html (need to remove html tags → extractHTMLStrip() in tm.plugin.webmining package), JSON, Scrape from the web → twitter, wikipedia, etc
  - Getting data off the web: get URL (could just have user copy/paste it in) → plug into read_html → save this as a file → you can do html_nodes(“div”) to get all text off of a page → html_text() gets just the text without the html stuff
    - More cleaning: http://bradleyboehmke.github.io/2015/12/scraping-html-text.html
    - May just want to write the regex removals into the code as a precaution to remove as much stuff as possible
  - Twitter: not sure how to pull tweets via an interface because you need your own api key and other information that has to be set up via the Twitter website
    - https://pushpullfork.com/mining-twitter-data-tidy-text-tags/
  - Need to write code for taking in just one document vs taking in multiple documents
    - User can upload as many as they want → just have an add more option
    - Read in data → convert to corpus → remove extra junk →optional: convert to document term matrix
      - For visualizing, we often keep the data in a tidy format
      - For actual analysis, we often need the matrix
    - One document doesn’t need to be converted to a corpus but multiple documents do
      - To use tidytext package, data doesn’t have to be in a corpus object BUT to use the tm package, it does have to be
      - CHECK: test doing the cleaning using both packages and see which one does a better job

What format does the data have to be converted to?
  - Maybe offer up a way to look at the file as it is read in and then allow the user to remove some lines that are useless (like I did with Peter Pan when I removed the table and contents and extra stuff in the beginning)
  - In my cleaning, I just stored which line the word was on, but what if you wanted to do something like chapters? 
    - I’m not sure that there is a universal way to do this → you’d use a regex expression to tell R where to cut it but I don’t think I could write something that would work for all chapter formats
  - Tidy format → basically one word per line (but could be multiple) while keeping some extra information (like what line that word appeared on, some document ID, etc)
    - unnest_tokens() function remove punctuation and automatically converts everything to lowercase → does it remove numbers and special characters?
  - Document term matrix format
    - Matrix with documents on one axis and terms that appear in the corpus on the other axis → values in the matrix are frequencies (with most entries being empty since each document doesn’t contain most of the terms)
    - What tidy format lends itself to being easily converted?
  - Probably want the default format to be one in which you can do the simple things like word counts and basic sentiment analysis → then I can code the format to change based on which visualizations the user chooses to display
    - For instance, there are quite a few visuals where the data has to be ‘spread’ to be in wide format for creating new variables and for plotting → BUT it shouldn’t be in this form unless you need it to be → so this type of thing would likely be written into the visualization code
  - Chapter 5 in tidytext: shows how to convert between document-term matrices and tidy data frames, as well as converting from a Corpus object to a text data frame 

What wrangling steps do you have to take with each type in order to clean them?
  - Load in the data and look at its raw format
    - If this is a text file with a book or something, there won’t be variables to mess with
      - But you could create variables like chapter or row number for indexing later on
    - What are the variables stored as? Ex: NASA case study that took in a JSON file and created took a few variables from it to create text files
      - Characters? Lists? Other formats?
      - What variable type do we want everything to be in to work? 
  - Tokenization
    - Have choices for one word, bigrams, sentences, etc
  - Do we want to stem words? → reduce them down to the root → would take care of things like adventures and adventure being counted as two separate things
  - Removing punctuation, numbers, stop words, everything to lower case, etc
  - Join on ‘extras’ → sentiment lexicons, total word counts 


Ideas for next steps…
  - Writing functions for taking in different file types
    - Try to cover as many bases as possible
    - Really just a lot of testing different types of files with varying structural differences to see how well they can all be cleaned
  - The main point for the next phase: write code that can “tidy” as many different inputs as possible
  
  
  
  
  Possible additions to Shiny:
  - Text Analytics in R for Students of Literature (Jockers) - look in library
    - LDA Buffet - learn about this for comps p. 163-167
    - Expository topic, application, interpretation
    - Find recent paper
    - Statistical Analysis and Data Mining - search here for articles (look @ table of contents)
  - Pre curated data 
    - Peter Pan - one file per chapter
    - Amherst College Course Catalog - one file per department
    - Emily Dickinson - one file per poem
  - Create option to clean up data

